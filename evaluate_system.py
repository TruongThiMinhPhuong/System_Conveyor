"""
ƒê√°nh Gi√° ƒê·ªô Ch√≠nh X√°c H·ªá Th·ªëng
Evaluation script cho Raspberry Pi
"""

import cv2
import numpy as np
import json
import time
from pathlib import Path
from collections import defaultdict
from datetime import datetime

from ai_models import YOLODetector, MobileNetClassifier, ImagePreprocessor
from utils import Config, PerformanceMonitor


class SystemEvaluator:
    """
    ƒê√°nh gi√° to√†n di·ªán ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng
    """
    
    def __init__(self, output_dir="evaluation_results"):
        """
        Initialize evaluator
        
        Args:
            output_dir: Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Load models
        self.detector = YOLODetector(
            model_path=Config.YOLO_MODEL_PATH,
            confidence_threshold=Config.YOLO_CONFIDENCE_THRESHOLD
        )
        self.classifier = MobileNetClassifier(
            model_path=Config.MOBILENET_MODEL_PATH
        )
        self.preprocessor = ImagePreprocessor(
            target_size=(Config.MOBILENET_INPUT_SIZE, Config.MOBILENET_INPUT_SIZE),
            blur_kernel=Config.BLUR_KERNEL_SIZE,
            fast_mode=Config.FAST_PREPROCESSING
        )
        
        # Performance monitor
        self.perf_monitor = PerformanceMonitor()
        
        # Statistics
        self.stats = defaultdict(int)
        self.results = []
        
    def load_models(self):
        """Load AI models"""
        print("üì¶ Loading models...")
        
        if not self.detector.load_model():
            raise Exception("Failed to load YOLO model")
        
        if not self.classifier.load_model():
            raise Exception("Failed to load MobileNet model")
        
        print("‚úÖ Models loaded successfully!")
        
    def evaluate_single_image(self, image_path, ground_truth_label):
        """
        ƒê√°nh gi√° 1 ·∫£nh
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh
            ground_truth_label: Nh√£n th·ª±c t·∫ø ('fresh' ho·∫∑c 'spoiled')
            
        Returns:
            Dict k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        # Load image
        image = cv2.imread(str(image_path))
        if image is None:
            return {'error': 'Cannot load image'}
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        result = {
            'image': str(image_path),
            'ground_truth': ground_truth_label,
            'timestamp': datetime.now().isoformat()
        }
        
        # Step 1: YOLO Detection
        start_time = time.time()
        detections = self.detector.detect(image_rgb, verbose=False)
        yolo_time = time.time() - start_time
        result['yolo_time_ms'] = yolo_time * 1000
        
        if not detections:
            result['detected'] = False
            result['correct'] = False
            self.stats['detection_failed'] += 1
            return result
        
        result['detected'] = True
        detection = max(detections, key=lambda x: x['confidence'])
        result['detection_confidence'] = detection['confidence']
        result['detection_class'] = detection['class_name']
        
        # Step 2: Preprocessing
        start_time = time.time()
        bbox = detection['bbox']
        preprocessed = self.preprocessor.preprocess_complete_pipeline(image_rgb, bbox)
        prep_time = time.time() - start_time
        result['preprocessing_time_ms'] = prep_time * 1000
        
        if preprocessed is None:
            result['classified'] = False
            result['correct'] = False
            self.stats['preprocessing_failed'] += 1
            return result
        
        # Step 3: Classification
        start_time = time.time()
        classification = self.classifier.classify_with_details(preprocessed)
        class_time = time.time() - start_time
        result['classification_time_ms'] = class_time * 1000
        
        result['classified'] = True
        result['predicted_class'] = classification['predicted_class']
        result['classification_confidence'] = classification['confidence']
        result['is_fresh'] = classification['is_fresh']
        
        # Total time
        result['total_time_ms'] = (yolo_time + prep_time + class_time) * 1000
        
        # Correctness check
        predicted = 'fresh' if classification['is_fresh'] else 'spoiled'
        result['correct'] = (predicted == ground_truth_label)
        
        # Update stats
        if result['correct']:
            self.stats['correct'] += 1
        else:
            self.stats['incorrect'] += 1
            
        self.stats['total'] += 1
        
        return result
    
    def evaluate_dataset(self, test_dir, batch_size=10, save_interval=50, max_images=None):
        """
        ƒê√°nh gi√° to√†n b·ªô dataset test v·ªõi batch processing v√† progress tracking

        Args:
            test_dir: Th∆∞ m·ª•c ch·ª©a ·∫£nh test
                      test_dir/fresh/*.jpg
                      test_dir/spoiled/*.jpg
            batch_size: S·ªë ·∫£nh x·ª≠ l√Ω m·ªói batch
            save_interval: L∆∞u k·∫øt qu·∫£ sau m·ªói N ·∫£nh
            max_images: Gi·ªõi h·∫°n s·ªë ·∫£nh x·ª≠ l√Ω (None = t·∫•t c·∫£)
        """
        test_path = Path(test_dir)

        print(f"\n{'='*60}")
        print(f"ƒê√°nh Gi√° Dataset: {test_dir}")
        print(f"{'='*60}\n")

        # Collect images
        fresh_images = list((test_path / 'fresh').glob('*.[jp][pn][g]*'))
        spoiled_images = list((test_path / 'spoiled').glob('*.[jp][pn][g]*'))

        print(f"üìä Dataset:")
        print(f"   Fresh: {len(fresh_images)} images")
        print(f"   Spoiled: {len(spoiled_images)} images")
        print(f"   Total: {len(fresh_images) + len(spoiled_images)} images")
        print(f"   Batch size: {batch_size}")
        print(f"   Save interval: {save_interval}")

        # Combine all images with labels
        all_images = [(img, 'fresh') for img in fresh_images] + [(img, 'spoiled') for img in spoiled_images]

        # Limit images if specified
        if max_images and len(all_images) > max_images:
            print(f"   Limited to: {max_images} images (quick test mode)")
            all_images = all_images[:max_images]

        total_images = len(all_images)

        # Progress tracking
        processed = 0
        start_time = time.time()

        print(f"\nüöÄ Starting evaluation of {total_images} images...")

        try:
            # Process in batches
            for i in range(0, total_images, batch_size):
                batch = all_images[i:i+batch_size]
                batch_start = time.time()

                print(f"\nüì¶ Processing batch {i//batch_size + 1}/{(total_images + batch_size - 1)//batch_size}")
                print(f"   Images {i+1}-{min(i+batch_size, total_images)} of {total_images}")

                for img_path, label in batch:
                    try:
                        result = self.evaluate_single_image(img_path, label)
                        self.results.append(result)
                        processed += 1

                        # Progress update every 5 images
                        if processed % 5 == 0:
                            elapsed = time.time() - start_time
                            rate = processed / elapsed if elapsed > 0 else 0
                            remaining = (total_images - processed) / rate if rate > 0 else 0
                            print(f"   üìä Progress: {processed}/{total_images} "
                                  f"({processed/total_images*100:.1f}%) - "
                                  f"{rate:.1f} img/s - ETA: {remaining:.0f}s")

                    except KeyboardInterrupt:
                        print(f"\n‚ö†Ô∏è  KeyboardInterrupt detected at image {processed+1}")
                        raise
                    except Exception as e:
                        print(f"   ‚ùå Error processing {img_path}: {e}")
                        self.stats['processing_errors'] += 1
                        continue

                batch_time = time.time() - batch_start
                print(f"   ‚úÖ Batch completed in {batch_time:.1f}s")

                # Save intermediate results
                if processed % save_interval == 0 and processed > 0:
                    print(f"   üíæ Saving intermediate results...")
                    self.calculate_metrics()
                    self.save_results(intermediate=True)

            # Final calculation and save
            print(f"\nüéØ Evaluation completed! Processed {processed}/{total_images} images")
            self.calculate_metrics()
            self.save_results()

        except KeyboardInterrupt:
            print(f"\n‚èπÔ∏è  Evaluation interrupted at {processed}/{total_images} images")
            print("üíæ Saving partial results...")
            self.calculate_metrics()
            self.save_results(intermediate=True, interrupted=True)
            raise
        
    def calculate_metrics(self):
        """T√≠nh to√°n c√°c metrics ƒë√°nh gi√°"""
        
        # Filter successful classifications
        classified_results = [r for r in self.results if r.get('classified', False)]
        
        if not classified_results:
            print("\n‚ö†Ô∏è No successful classifications!")
            return
        
        # Confusion matrix components
        tp_fresh = sum(1 for r in classified_results 
                      if r['ground_truth'] == 'fresh' and r['is_fresh'])
        tn_spoiled = sum(1 for r in classified_results 
                        if r['ground_truth'] == 'spoiled' and not r['is_fresh'])
        fp_fresh = sum(1 for r in classified_results 
                      if r['ground_truth'] == 'spoiled' and r['is_fresh'])
        fn_spoiled = sum(1 for r in classified_results 
                        if r['ground_truth'] == 'fresh' and not r['is_fresh'])
        
        # Metrics
        total = len(classified_results)
        accuracy = (tp_fresh + tn_spoiled) / total if total > 0 else 0
        
        # Precision & Recall for Fresh
        precision_fresh = tp_fresh / (tp_fresh + fp_fresh) if (tp_fresh + fp_fresh) > 0 else 0
        recall_fresh = tp_fresh / (tp_fresh + fn_spoiled) if (tp_fresh + fn_spoiled) > 0 else 0
        f1_fresh = 2 * (precision_fresh * recall_fresh) / (precision_fresh + recall_fresh) \
                   if (precision_fresh + recall_fresh) > 0 else 0
        
        # Precision & Recall for Spoiled  
        precision_spoiled = tn_spoiled / (tn_spoiled + fn_spoiled) if (tn_spoiled + fn_spoiled) > 0 else 0
        recall_spoiled = tn_spoiled / (tn_spoiled + fp_fresh) if (tn_spoiled + fp_fresh) > 0 else 0
        f1_spoiled = 2 * (precision_spoiled * recall_spoiled) / (precision_spoiled + recall_spoiled) \
                     if (precision_spoiled + recall_spoiled) > 0 else 0
        
        # Average times
        avg_yolo_time = np.mean([r.get('yolo_time_ms', 0) for r in classified_results])
        avg_prep_time = np.mean([r.get('preprocessing_time_ms', 0) for r in classified_results])
        avg_class_time = np.mean([r.get('classification_time_ms', 0) for r in classified_results])
        avg_total_time = np.mean([r.get('total_time_ms', 0) for r in classified_results])
        
        # Average confidences
        avg_det_conf = np.mean([r.get('detection_confidence', 0) for r in classified_results])
        avg_class_conf = np.mean([r.get('classification_confidence', 0) for r in classified_results])
        
        # Store metrics
        self.metrics = {
            'confusion_matrix': {
                'true_positive_fresh': tp_fresh,
                'true_negative_spoiled': tn_spoiled,
                'false_positive_fresh': fp_fresh,
                'false_negative_spoiled': fn_spoiled
            },
            'accuracy': accuracy,
            'fresh': {
                'precision': precision_fresh,
                'recall': recall_fresh,
                'f1_score': f1_fresh
            },
            'spoiled': {
                'precision': precision_spoiled,
                'recall': recall_spoiled,
                'f1_score': f1_spoiled
            },
            'performance': {
                'avg_yolo_time_ms': avg_yolo_time,
                'avg_preprocessing_time_ms': avg_prep_time,
                'avg_classification_time_ms': avg_class_time,
                'avg_total_time_ms': avg_total_time,
                'estimated_fps': 1000 / avg_total_time if avg_total_time > 0 else 0
            },
            'confidences': {
                'avg_detection_confidence': avg_det_conf,
                'avg_classification_confidence': avg_class_conf
            }
        }
        
    def print_results(self):
        """In k·∫øt qu·∫£ ƒë√°nh gi√°"""
        
        if not hasattr(self, 'metrics'):
            print("\n‚ö†Ô∏è No metrics calculated yet!")
            return
        
        m = self.metrics
        
        print(f"\n{'='*60}")
        print("üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å H·ªÜ TH·ªêNG")
        print(f"{'='*60}\n")
        
        # Overall stats
        print(f"üìà T·ªïng Quan:")
        print(f"   T·ªïng s·ªë ·∫£nh: {self.stats['total']}")
        print(f"   Ph√°t hi·ªán th√†nh c√¥ng: {self.stats['total'] - self.stats.get('detection_failed', 0)}")
        print(f"   Ph√¢n lo·∫°i ch√≠nh x√°c: {self.stats['correct']}")
        print(f"   Ph√¢n lo·∫°i sai: {self.stats['incorrect']}")
        
        # Accuracy metrics
        print(f"\nüéØ ƒê·ªô Ch√≠nh X√°c:")
        print(f"   Overall Accuracy: {m['accuracy']:.2%}")
        
        print(f"\nüçè Fresh Class:")
        print(f"   Precision: {m['fresh']['precision']:.2%}")
        print(f"   Recall: {m['fresh']['recall']:.2%}")
        print(f"   F1 Score: {m['fresh']['f1_score']:.2%}")
        
        print(f"\nüçé Spoiled Class:")
        print(f"   Precision: {m['spoiled']['precision']:.2%}")
        print(f"   Recall: {m['spoiled']['recall']:.2%}")
        print(f"   F1 Score: {m['spoiled']['f1_score']:.2%}")
        
        # Confusion Matrix
        cm = m['confusion_matrix']
        print(f"\nüìã Confusion Matrix:")
        print(f"                 Predicted Fresh  |  Predicted Spoiled")
        print(f"   Actual Fresh:     {cm['true_positive_fresh']:3d}         |       {cm['false_negative_spoiled']:3d}")
        print(f"   Actual Spoiled:   {cm['false_positive_fresh']:3d}         |       {cm['true_negative_spoiled']:3d}")
        
        # Performance
        perf = m['performance']
        print(f"\n‚ö° Hi·ªáu NƒÉng (Raspberry Pi):")
        print(f"   YOLO Detection: {perf['avg_yolo_time_ms']:.1f}ms")
        print(f"   Preprocessing: {perf['avg_preprocessing_time_ms']:.1f}ms")
        print(f"   Classification: {perf['avg_classification_time_ms']:.1f}ms")
        print(f"   Total: {perf['avg_total_time_ms']:.1f}ms")
        print(f"   Estimated FPS: {perf['estimated_fps']:.1f}")
        
        # Confidences
        conf = m['confidences']
        print(f"\nüîç ƒê·ªô Tin C·∫≠y:")
        print(f"   Avg Detection Confidence: {conf['avg_detection_confidence']:.2%}")
        print(f"   Avg Classification Confidence: {conf['avg_classification_confidence']:.2%}")
        
        print(f"\n{'='*60}\n")
        
        # Assessment
        self.print_assessment()
        
    def print_assessment(self):
        """ƒê√°nh gi√° k·∫øt qu·∫£"""
        
        m = self.metrics
        accuracy = m['accuracy']
        f1_fresh = m['fresh']['f1_score']
        f1_spoiled = m['spoiled']['f1_score']
        fps = m['performance']['estimated_fps']
        
        print("üéì ƒê√ÅNH GI√Å:")
        print()
        
        # Accuracy assessment
        if accuracy >= 0.95:
            print("   ‚úÖ Accuracy: XU·∫§T S·∫ÆC (‚â•95%)")
        elif accuracy >= 0.90:
            print("   ‚úÖ Accuracy: T·ªêT (‚â•90%)")
        elif accuracy >= 0.85:
            print("   ‚ö†Ô∏è  Accuracy: KH√Å (<90%, c·∫ßn c·∫£i thi·ªán)")
        else:
            print("   ‚ùå Accuracy: TH·∫§P (<85%, c·∫ßn train l·∫°i)")
        
        # F1 score assessment
        avg_f1 = (f1_fresh + f1_spoiled) / 2
        if avg_f1 >= 0.90:
            print("   ‚úÖ F1 Score: T·ªêT (‚â•90%)")
        elif avg_f1 >= 0.85:
            print("   ‚ö†Ô∏è  F1 Score: KH√Å (<90%)")
        else:
            print("   ‚ùå F1 Score: TH·∫§P (<85%)")
        
        # Performance assessment
        if fps >= 10:
            print("   ‚úÖ Performance: ƒê·ª¶ NHANH (‚â•10 FPS)")
        elif fps >= 8:
            print("   ‚ö†Ô∏è  Performance: CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C (‚â•8 FPS)")
        else:
            print("   ‚ùå Performance: QU√Å CH·∫¨M (<8 FPS)")
        
        print()
        
        # Recommendations
        if accuracy < 0.90 or avg_f1 < 0.90:
            print("üí° KHUY·∫æN NGH·ªä C·∫¢I THI·ªÜN:")
            print("   - Thu th·∫≠p th√™m d·ªØ li·ªáu (200+ ·∫£nh/lo·∫°i)")
            print("   - ƒê·∫£m b·∫£o ·∫£nh ƒëa d·∫°ng (g√≥c ƒë·ªô, √°nh s√°ng)")
            print("   - Train l·∫°i v·ªõi epochs cao h∆°n")
            print("   - Ki·ªÉm tra quality dataset")
        
        if fps < 10:
            print("üí° KHUY·∫æN NGH·ªä T·ªêI ∆ØU:")
            print("   - Gi·∫£m CAMERA_RESOLUTION xu·ªëng 320x320")
            print("   - Set FAST_PREPROCESSING = True")
            print("   - Ki·ªÉm tra XNNPACK delegate")
            print("   - Xem x√©t d√πng Pi 5 ho·∫∑c Coral TPU")
    
    def save_results(self, intermediate=False, interrupted=False):
        """L∆∞u k·∫øt qu·∫£ ra file"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix = "_intermediate" if intermediate else ("_interrupted" if interrupted else "")

        # Save detailed results
        results_file = self.output_dir / f"evaluation_{timestamp}{suffix}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'intermediate': intermediate,
                    'interrupted': interrupted,
                    'total_images_processed': len(self.results)
                },
                'metrics': getattr(self, 'metrics', {}),
                'stats': dict(self.stats),
                'results': self.results
            }, f, indent=2, ensure_ascii=False)

        print(f"üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u: {results_file}")

        # Save summary report only for final results
        if not intermediate and hasattr(self, 'metrics'):
            report_file = self.output_dir / f"report_{timestamp}{suffix}.txt"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\n")
                f.write("B√ÅO C√ÅO ƒê√ÅNH GI√Å H·ªÜ TH·ªêNG\n")
                f.write("="*60 + "\n\n")
                f.write(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                if intermediate:
                    f.write("TR·∫†NG TH√ÅI: K·∫æT QU·∫¢ T·∫†M TH·ªúI\n\n")
                elif interrupted:
                    f.write("TR·∫†NG TH√ÅI: ƒê√É D·ª™NG (INTERRUPTED)\n\n")
                else:
                    f.write("TR·∫†NG TH√ÅI: HO√ÄN TH√ÄNH\n\n")
                f.write(f"S·ªë ·∫£nh ƒë√£ x·ª≠ l√Ω: {len(self.results)}\n")
                f.write(f"Accuracy: {self.metrics.get('accuracy', 0):.2%}\n")
                f.write(f"F1 Fresh: {self.metrics.get('fresh', {}).get('f1_score', 0):.2%}\n")
                f.write(f"F1 Spoiled: {self.metrics.get('spoiled', {}).get('f1_score', 0):.2%}\n")
                f.write(f"Avg FPS: {self.metrics.get('performance', {}).get('estimated_fps', 0):.1f}\n")

            print(f"üìÑ B√°o c√°o ƒë√£ l∆∞u: {report_file}")


def main():
    """Main evaluation function"""
    import argparse

    parser = argparse.ArgumentParser(description="ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c h·ªá th·ªëng")
    parser.add_argument('--test_dir', type=str, required=True,
                       help="Th∆∞ m·ª•c ch·ª©a ·∫£nh test (c√≥ subfolder fresh/ v√† spoiled/)")
    parser.add_argument('--output', type=str, default='evaluation_results',
                       help="Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£")
    parser.add_argument('--batch_size', type=int, default=10,
                       help="S·ªë ·∫£nh x·ª≠ l√Ω m·ªói batch (default: 10)")
    parser.add_argument('--save_interval', type=int, default=50,
                       help="L∆∞u k·∫øt qu·∫£ sau m·ªói N ·∫£nh (default: 50)")
    parser.add_argument('--quick_test', action='store_true',
                       help="Ch·∫°y test nhanh v·ªõi √≠t ·∫£nh (20 ·∫£nh ƒë·∫ßu ti√™n)")

    args = parser.parse_args()

    # Create evaluator
    evaluator = SystemEvaluator(output_dir=args.output)

    # Load models
    evaluator.load_models()

    # Run evaluation
    try:
        if args.quick_test:
            print("\nüß™ QUICK TEST MODE: Ch·ªâ x·ª≠ l√Ω 20 ·∫£nh ƒë·∫ßu ti√™n")
            evaluator.evaluate_dataset(args.test_dir, batch_size=5, save_interval=10, max_images=20)
        else:
            evaluator.evaluate_dataset(args.test_dir, args.batch_size, args.save_interval)

        # Print results
        evaluator.print_results()

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  ƒê√£ d·ª´ng ƒë√°nh gi√° theo y√™u c·∫ßu ng∆∞·ªùi d√πng")
        print("K·∫øt qu·∫£ t·∫°m th·ªùi ƒë√£ ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông")


if __name__ == "__main__":
    main()
