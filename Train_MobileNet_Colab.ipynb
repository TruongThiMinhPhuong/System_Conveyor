{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ AI Fruit Sorter - MobileNet Training\n",
    "## Train Fresh/Spoiled Classification Model (Miá»…n PhÃ­ vá»›i GPU)\n",
    "\n",
    "### ğŸ“‹ HÆ°á»›ng Dáº«n\n",
    "1. **Runtime > Change runtime type** â†’ chá»n **T4 GPU** (miá»…n phÃ­)\n",
    "2. Cháº¡y tá»«ng cell theo thá»© tá»± (Shift+Enter)\n",
    "3. Upload dataset cá»§a báº¡n khi Ä‘Æ°á»£c yÃªu cáº§u\n",
    "4. Chá» training xong (~15-20 phÃºt)\n",
    "5. Download file `.tflite` vá» Raspberry Pi\n",
    "\n",
    "### âš¡ GPU Miá»…n PhÃ­!\n",
    "Colab cung cáº¥p GPU miá»…n phÃ­ â†’ Train nhanh hÆ¡n 10-20 láº§n so vá»›i CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow opencv-python matplotlib scikit-learn seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"âœ… TensorFlow version:\", tf.__version__)\n",
    "print(\"âœ… GPU available:\", \"YES\" if tf.config.list_physical_devices('GPU') else \"NO (using CPU)\")\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p dataset/train/fresh dataset/train/spoiled\n",
    "!mkdir -p dataset/val/fresh dataset/val/spoiled\n",
    "!mkdir -p dataset/test/fresh dataset/test/spoiled\n",
    "!mkdir -p output\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Upload Dataset\n",
    "\n",
    "### CÃ¡ch 1: Upload File ZIP\n",
    "Chuáº©n bá»‹ file ZIP vá»›i cáº¥u trÃºc:\n",
    "```\n",
    "dataset.zip\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ fresh/\n",
    "â”‚   â”‚   â”œâ”€â”€ img1.jpg\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â””â”€â”€ spoiled/\n",
    "â”‚       â””â”€â”€ ...\n",
    "â”œâ”€â”€ val/\n",
    "â”‚   â”œâ”€â”€ fresh/\n",
    "â”‚   â””â”€â”€ spoiled/\n",
    "â””â”€â”€ test/\n",
    "    â”œâ”€â”€ fresh/\n",
    "    â””â”€â”€ spoiled/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"ğŸ“ Upload file ZIP dataset cá»§a báº¡n...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract ZIP\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nğŸ“¦ Extracting {filename}...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('dataset')\n",
    "    print(\"âœ… Extracted!\")\n",
    "\n",
    "# Count images\n",
    "def count_images(path):\n",
    "    return len(list(Path(path).glob('*.[jp][pn][g]*')))\n",
    "\n",
    "train_fresh = count_images('dataset/train/fresh')\n",
    "train_spoiled = count_images('dataset/train/spoiled')\n",
    "val_fresh = count_images('dataset/val/fresh')\n",
    "val_spoiled = count_images('dataset/val/spoiled')\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "print(f\"   Train: {train_fresh} fresh, {train_spoiled} spoiled\")\n",
    "print(f\"   Val: {val_fresh} fresh, {val_spoiled} spoiled\")\n",
    "print(f\"   Total: {train_fresh + train_spoiled + val_fresh + val_spoiled} images\")\n",
    "\n",
    "if train_fresh < 20 or train_spoiled < 20:\n",
    "    print(\"\\nâš ï¸ Warning: Ãt áº£nh! KhuyÃªn dÃ¹ng Ã­t nháº¥t 50 áº£nh má»—i loáº¡i\")\n",
    "else:\n",
    "    print(\"\\nâœ… Dataset OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Data Augmentation (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_augmentation_pipeline():\n",
   "    \"\"\"Enhanced augmentation for conveyor belt conditions\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomRotation(0.07),  # 25 degrees\n",
    "        layers.RandomZoom(0.25),\n",
    "        layers.RandomTranslation(0.25, 0.25, fill_mode='reflect'),\n",
    "        layers.RandomBrightness((-0.4, 0.4)),\n",
    "        layers.RandomContrast(0.3),\n",
    "        layers.Rescaling(1./127.5, offset=-1)  # Normalize to [-1, 1]\n",
    "    ], name=\"augmentation\")\n",
    "\n",
    "def create_data_generator(directory, image_size=(224, 224), batch_size=32, shuffle=True, augment=False):\n",
    "    \"\"\"Create data generator\"\"\"\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        class_names=['fresh', 'spoiled'],\n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    if augment:\n",
    "        augmentation = create_augmentation_pipeline()\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (augmentation(x, training=True), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    else:\n",
    "        normalization = layers.Rescaling(1./127.5, offset=-1)\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (normalization(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"âœ… Augmentation pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Create Model (With Optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "def create_mobilenet_model(input_shape=(224, 224, 3), num_classes=2, dropout_rate=0.5):\n",
    "    \"\"\"Create optimized MobileNetV2 model\"\"\"\n",
    "    \n",
    "    # Load pretrained base\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze base\n",
    "    \n",
    "    # Build model\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    x = layers.Dense(\n",
    "        256, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(\n",
    "        128, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_mobilenet_model()\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nâœ… Model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Prepare Data & Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = create_data_generator(\n",
    "    'dataset/train',\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = create_data_generator(\n",
    "    'dataset/val',\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Calculate class weights\n",
    "total_count = train_fresh + train_spoiled\n",
    "weight_fresh = total_count / (2 * train_fresh) if train_fresh > 0 else 1.0\n",
    "weight_spoiled = total_count / (2 * train_spoiled) if train_spoiled > 0 else 1.0\n",
    "class_weights = {0: weight_fresh, 1: weight_spoiled}\n",
    "\n",
    "print(f\"âš–ï¸ Class weights: Fresh={weight_fresh:.3f}, Spoiled={weight_spoiled:.3f}\")\n",
    "print(\"âœ… Data ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Train Model ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'output/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train!\n",
    "EPOCHS = 50\n",
    "\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Using GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "results = model.evaluate(val_dataset, verbose=0)\n",
    "\n",
    "print(\"ğŸ“ˆ Validation Metrics:\")\n",
    "print(f\"   Loss: {results[0]:.4f}\")\n",
    "print(f\"   Accuracy: {results[1]:.2%}\")\n",
    "print(f\"   Precision: {results[2]:.2%}\")\n",
    "print(f\"   Recall: {results[3]:.2%}\")\n",
    "\n",
    "# Calculate F1\n",
    "precision = results[2]\n",
    "recall = results[3]\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"   F1 Score: {f1:.2%}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[0, 0].set_title('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Val')\n",
    "axes[0, 1].set_title('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val')\n",
    "axes[1, 0].set_title('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val')\n",
    "axes[1, 1].set_title('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Convert to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = tf.keras.models.load_model('output/best_model.keras')\n",
    "\n",
    "# Convert to TFLite with optimizations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]  # Smaller size\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "tflite_path = 'output/mobilenet_classifier.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Size info\n",
    "keras_size = os.path.getsize('output/best_model.keras') / 1024 / 1024\n",
    "tflite_size = os.path.getsize(tflite_path) / 1024 / 1024\n",
    "\n",
    "print(\"ğŸ“¦ Model Conversion:\")\n",
    "print(f\"   Keras model: {keras_size:.2f} MB\")\n",
    "print(f\"   TFLite model: {tflite_size:.2f} MB\")\n",
    "print(f\"   Size reduction: {(1 - tflite_size/keras_size)*100:.1f}%\")\n",
    "print(f\"\\nâœ… TFLite model saved: {tflite_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Download Model ğŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"ğŸ“¥ Downloading model...\")\n",
    "print(\"\\nFiles sáº½ táº£i vá»:\")\n",
    "print(\"  1. mobilenet_classifier.tflite â† Copy vÃ o Raspberry Pi\")\n",
    "print(\"  2. best_model.keras â† Backup\")\n",
    "print(\"  3. training_history.png â† Biá»ƒu Ä‘á»“ training\")\n",
    "\n",
    "files.download('output/mobilenet_classifier.tflite')\n",
    "files.download('output/best_model.keras')\n",
    "files.download('output/training_history.png')\n",
    "\n",
    "print(\"\\nâœ… Download complete!\")\n",
    "print(\"\\nğŸ“‹ Next Steps:\")\n",
    "print(\"1. Copy mobilenet_classifier.tflite to Raspberry Pi:\")\n",
    "print(\"   scp mobilenet_classifier.tflite pi@192.168.137.177:~/System_Conveyor/models/\")\n",
    "print(\"\\n2. On Raspberry Pi, run:\")\n",
    "print(\"   cd ~/System_Conveyor\")\n",
    "print(\"   python3 fruit_sorter.py\")\n",
    "print(\"\\nğŸ‰ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… HoÃ n ThÃ nh!\n",
    "\n",
    "### ğŸ“Š Káº¿t Quáº£ Mong Äá»£i\n",
    "- Accuracy: > 90%\n",
    "- F1 Score: > 90%\n",
    "- Model size: ~3-4 MB\n",
    "\n",
    "### ğŸš€ Deploy to Raspberry Pi\n",
    "1. Download file `mobilenet_classifier.tflite`\n",
    "2. Copy to Raspberry Pi:\n",
    "   ```bash\n",
    "   scp mobilenet_classifier.tflite pi@192.168.137.177:~/System_Conveyor/models/\n",
    "   ```\n",
    "3. Run on Pi:\n",
    "   ```bash\n",
    "   cd ~/System_Conveyor\n",
    "   python3 fruit_sorter.py\n",
    "   ```\n",
    "\n",
    "### ğŸ’¡ Tips\n",
    "- If accuracy < 85%: Thu tháº­p thÃªm áº£nh\n",
    "- If overfitting: TÄƒng augmentation hoáº·c dropout\n",
    "- Äá»ƒ train láº¡i: Runtime > Restart and run all\n",
    "\n",
    "**ğŸ‰ ChÃºc báº¡n thÃ nh cÃ´ng!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
